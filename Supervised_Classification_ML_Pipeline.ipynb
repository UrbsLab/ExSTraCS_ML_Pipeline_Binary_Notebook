{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Rigorous Machine Learning Pipeline (Supervised Binary Classification):\n",
    "* Author: Ryan Urbanowicz, PhD \n",
    "*  Affiliation: University of Pennsylvania - Department of Biostatistics, Epidemiology, and Informatics & Institute for Biomedical Informatics (IBI) \n",
    "* Date: 8/25/20\n",
    "***\n",
    "## Introduction\n",
    "This notebook presents an example of a 'rigorous' machine learning (ML) analysis pipeline that could be reasonablly applied to various supervised learning classification tasks. This pipeline was developed with biomedical data mining/modeling in mind. While this pipeline offers a comprehensive and rigorous proposed analysis, we do not suggest that this is necessarily the 'best' or 'only' way to conduct an appropriate binary classification ML analysis. \n",
    "***\n",
    "## Environment Requirements\n",
    "In order to run this pipeline as a Jupyter Notebook you must have the proper environment set up on your computer. Python 3 as well as a number of Python packages are required.  Most of these requirements are satisfied by installing the most recent version of anaconda (https://docs.anaconda.com/anaconda/install/). We used Anaconda3 with python version 3.7.7 during this pipeline development. In addition to the packages included in anaconda, the following packages will need to be installed separately (or possibly updated, if you have an older version installed):\n",
    "\n",
    "* scikit-rebate (To install: pip install skrebate)\n",
    "* xgboost (To install: pip install xgboost)\n",
    "* lightgbm (To install: pip install lightgbm)\n",
    "* optuna (To install: pip install optuna)\n",
    "\n",
    "Additionally, while currently commented out in the file (modeling_methods.py) if you want the optuna hypterparameter sweep figures to appear within the jupyter notebook (via the command 'fig.show()' ) you will need to run the following installation commands.  This should only be required if you edit the python file to uncomment this line for any or all of the ML modeling algorithms. \n",
    "\n",
    "* pip install -U plotly>=4.0.0\n",
    "* conda install -c plotly plotly-orca\n",
    "\n",
    "Lastly, in order to include the stand-alone algorithm 'ExSTraCS' we needed to call this from the command line within this Jupyter Notebook.  As a result, the part of this notebook running ExSTraCS will only run properly if the path to the working directory used to run this notebook includes no spaces.  In other words if your path includes a folder called 'My Folder' vs. 'My_Folder' you will likely get a run error for ExSTraCS (at least on a Windows machine). Thus, make sure to check that wherever you are running this notebook from, that the entire path to the working directory does note include any spaces. \n",
    "***\n",
    "## Dataset Requirements\n",
    "This notebook loads a single dataset to be run through the entire pipeline. Here we summarize the requirements for this dataset:\n",
    "* Ensure your data is in a single file: (If you have a pre-partitioned training/testing dataset, you should combine them into a single dataset before running this notebook)\n",
    "* Any dataset specific cleaning, feature transformation, or feature engineering that may be needed in order to maximize ML performance should be conducted by the user separately or added to the beginning of this notebook. \n",
    "* The dataset should be in tab-delimited .txt format to run this notebook (as is).  Commented-out code to load a comma separated file (.csv) and excel file (.xlsx) is included in the notebook as an alternative. \n",
    "* Missing data values should be empty or indicated with an 'NA'.\n",
    "* Dataset includes a header with column names. This should include a column for the binary class label and (optionally) a column for the instance ID, as well as columns for other 'features', e.g. independend variables. \n",
    "* The class labels should be 0 for the major class (i.e. the most frequent class), and 1 for the minor class.  This is important for generation of the precision/recall curve (PRC) plots. \n",
    "* This dataset is saved in the working directory containing the jupyter notebook file, and all other files in this repository.\n",
    "* All variables in the dataset have been numerically encoded (otherwise additional data preprocessing may be needed)\n",
    "***\n",
    "## Notebook Organization\n",
    "#### Part 1: Exploratory analysis, data cleaning, and creating n-fold CV partitioned datasets \n",
    "- Instances missing a class value are excluded\n",
    "- The user can indicate other columns that should be excluded from the analysis\n",
    "- The user can turn on/off the option to apply standard scaling to the data prior to CV partitioning or imputation\n",
    "    - We use no scaling by default. This is because most methods should work properly without it, and in applying the model downstream, it is difficult to properly scale new data so that models may be re-applied later.\n",
    "    - ANN modeling is sensitive to feature scaling, thus without it, performance not be as good. However this is only one of many challenges in getting ANN to perform well. \n",
    "- The user can turn on/off the option to impute missing values following CV partitioning\n",
    "- The user can turn on/off the option for the code to automatically attempt to discriminate nominal from ordinal features\n",
    "- The user can choose the number of CV partitions as well as the strategy for CV partitioning (i.e.  random (R), stratified (S), and matched (M) \n",
    "- CV training and testing datasets are saved as .txt files so that the same partitions may be analyzed external to this code\n",
    "    \n",
    "#### Part 2: Feature selection\n",
    "- The user can turn on/off the option to filter out the lowest scoring features in the data (i.e. to conduct not just feature importance evaluation but feature selection)\n",
    "- Feature importance evaluation and feature selection are conducted within each respective CV training partition\n",
    "- The pipeline reports feature importance estimates via two feature selection algorithms:\n",
    "    - Mutual Information: Proficient at detecting univariate associations\n",
    "    - MultiSURF: Proficient at detecting univariate associations, 2-way epistatic interactions, and heterogeneous associations\n",
    "    \n",
    "- When selected by the user, feature selection conservatively keeps any feature identified as 'potentially relevant' (i.e. score > 0) by either algorithm\n",
    "- Since MultiSURF scales quadratically with the number of training instances, there is an option to utilize a random subset of instances when running this algorithm to save computational time\n",
    "    \n",
    "#### Part 3: Machine learning modeling\n",
    "- Seven ML modeling algorithms have been implemented in this pipeline:\n",
    "    - Logistic Regression (scikit learn)\n",
    "    - Decision Tree (scikit learn)\n",
    "    - Random Forest (scikit learn)\n",
    "    - Naïve Bayes (scikit learn)\n",
    "    - XGBoost (separate python package)\n",
    "    - LightGBM (separate python package)\n",
    "    - SVM (scikit learn)\n",
    "    - ANN (scikit learn)\n",
    "    - ExSTraCS (v2.0.2.1) - a Learning Classifier System (LCS) algorithm manually configured to run in this notebook\n",
    "- User can select any subset of these methods to run\n",
    "- ML modeling is conducted within each respective CV training partition on the respective feature subset selected within the given CV partition\n",
    "- ML modeling begins with a hyperparameter sweep conducted with a grid search of hard coded run parameter options (user can edit as needed)\n",
    "- Balanced accuracy is applied as the evaluation metric for the hyperparameter sweep\n",
    "\n",
    "#### Part 4: ML feature importance vizualization\n",
    "\n",
    "***\n",
    "## Schematic of ML Analysis Pipeline\n",
    "\n",
    "<img src=\"ML pipeline schematic2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Necessary Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from data_processing_methods import cv_partitioner\n",
    "from data_processing_methods import identifyCategoricalFeatures\n",
    "from data_processing_methods import imputeCVData\n",
    "from data_processing_methods import dataScaling\n",
    "\n",
    "from feature_selection_methods import reportAllFS\n",
    "from feature_selection_methods import reportTopFS\n",
    "from feature_selection_methods import sort_save_fi_scores\n",
    "from feature_selection_methods import sort_save_fs_fi_scores\n",
    "from feature_selection_methods import run_mi\n",
    "from feature_selection_methods import run_multisurf\n",
    "from feature_selection_methods import selectFeatures\n",
    "from feature_selection_methods import genFilteredDatasets\n",
    "\n",
    "from modeling_methods import classEval\n",
    "from modeling_methods import roc_plot_single\n",
    "from modeling_methods import save_performance\n",
    "from modeling_methods import save_FI\n",
    "from modeling_methods import eval_Algorithm_FI\n",
    "from modeling_methods import eval_LCS\n",
    "from modeling_methods import eval_LCS_QRF\n",
    "\n",
    "#LCS package\n",
    "#sys.path.append('C:/home/work/research_urbslab/Core_ML_Pipeline/ml_pipeline_lcs/exstracs_2.0.2.1_noclassmutate_lynch')\n",
    "#import exstracs_main\n",
    "\n",
    "#Statistics packages\n",
    "from scipy import stats\n",
    "\n",
    "#Visualization Packages:\n",
    "#This code ensures that the output of plotting commands is displayed inline directly below the code cell that produced it.\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None) # display all the columns\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import randint\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#Import Progress bar:\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set Dataset Pipeline Variables (Mandatory)\n",
    "These variables values will have to be respecified to run this pipeline on a given dataset or computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique dataset name (include dataset extension) (must be updated for a given dataset analysis)\n",
    "target_data = 'hcc-data_example.txt' \n",
    "\n",
    "#Dataset specific constants (Must be updated for a given dataset)\n",
    "outcomeLabel = 'Class Attribute' #i.e. class outcome column label\n",
    "categoricalOutcome = True  #Is outcome nominal (i.e. discrete/classification) This script will not function correctly if False!\n",
    "instLabel = None #If data includes instance labels, given respective column name here, otherwise put None\n",
    "ignore_columns = [] #list of column names to exclude from the analysis (only insert column names if needed, otherwise leave empty)\n",
    "categorical_variables = [] #Leave empty for 'auto-detect' otherwise list feature names to be treated as categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set Other Pipeline Variables (Optional) \n",
    "These variables can be left as is, or modified to update key aspects of how the ML pipeline will be run and what methods will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive Analysis options ------------------------------------------------------------------------------------\n",
    "doCorrelation = True #Generate visualization of feature correlations (Not recommended for feature sets > 100)\n",
    "doUA = True #Perform a univariate analysis of features in the dataset \n",
    "\n",
    "#Cross Validation run parameters (change as needed) --------------------------------------------------------------\n",
    "cv_partitions = 10 #number of CV partitions (3-10 recommended)\n",
    "partition_method = 'S' #Options: random (R), stratified (S), matched (M)\n",
    "matchName = None #Name of column used for matched CV partitioning. Put None otherwise.\n",
    "\n",
    "#Data cleaning options--------------------------------------------------------------------------------------------\n",
    "scaleData = True  #Prescale original dataset values (important for ML and feature importance interpretation of some algorithms)\n",
    "imputeData = True  #Impute missing values (separate methods used for categorical vs ordinal variables)\n",
    "autoDetectCategorical = True #keep in mind that this can incorectly identify some ordinal values as being categorical (e.g. tumor state/grade)\n",
    "categoricalCutoff = 10 #If the number of unique values is greater than this value, the feature is assumed to be ordinal/continuous-valued\n",
    "\n",
    "#Feature Selection----------------------------------------------------------------------------------------------\n",
    "algorithms = ['mutual_information','multisurf']\n",
    "instanceSubset = 2000 #Sample subset size to use with MultiSURF (since they scale quandratically with n)\n",
    "maxFeaturesToKeep = 50 #Maximum number of features to keep during feature selection - pute None if no max desired)\n",
    "filterPoorFeatures = True\n",
    "topResults = 20 #Top features to illustrate in feature selection results figures\n",
    "\n",
    "#ML Modeling -----------------------------------------------------------------------------------------------------\n",
    "#To use all available algorithms, use the following (feel free to adjust the colors):\n",
    "#algorithmsToRun = ['logistic_regression','decision_tree', 'random_forest', 'naive_bayes', 'XGB', 'LGB','SVM','ANN','LCS','LCS_QRF'] \n",
    "#methodsKey = ('Logistic Regression', 'Decision Tree', 'Random Forest', 'Naïve Bayes','XGB','LGB', 'SVM', 'ANN','LCS','LCS_QRF')\n",
    "#algColors = ['black','yellow','orange','grey','purple', 'aqua', 'red', 'pink', 'green', 'blue']\n",
    "\n",
    "algorithmsToRun = ['logistic_regression','decision_tree', 'random_forest', 'naive_bayes', 'XGB', 'LGB','SVM','ANN','LCS','LCS_QRF'] \n",
    "methodsKey = ('Logistic Regression', 'Decision Tree', 'Random Forest', 'Naïve Bayes','XGB','LGB', 'SVM', 'ANN','LCS','LCS_QRF')\n",
    "algColors = ['black','yellow','orange','grey','purple', 'aqua', 'red', 'pink', 'green', 'blue']\n",
    "\n",
    "#Note that the statistical comparisons and compound feature importance plot sections of this notebook will fail if only one modeling algorithm is selected here.\n",
    "#To run a single ML modeling algorithm, use the following (example):\n",
    "#algorithmsToRun = ['decision_tree'] \n",
    "#methodsKey = ('Decision Tree')\n",
    "#algColors = ['black']\n",
    "\n",
    "#Notebook run parameters (i.e. set random seed for notebook replication consistency)\n",
    "randomSeed = 42\n",
    "random.seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Modeling Hyperparamters (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optima Hyperparameter sweep run parameters\n",
    "n_trials = 100 #number of bayesian hyperparameter optimization trials (using optuna)\n",
    "scoring_metric = 'balanced_accuracy' #metric used to pick best hyperparamters\n",
    "hype_cv = 3 #Internal cv partitions for hyperparamter sweep evaluations\n",
    "timeout = 300 #Seconds until hyperparameter sweep stopped - 3600 sec = 1 hour, 300 sec = 5 min\n",
    "\n",
    "#Logistic Regression\n",
    "param_grid_LR = {'penalty' : ['l2','l1'],\n",
    "                'C' : [1e-5, 1e5], \n",
    "                'dual' : [True, False],\n",
    "                'solver' : ['newton-cg','lbfgs', 'liblinear', 'sag','saga'],\n",
    "                'class_weight' : [None, 'balanced'],\n",
    "                'max_iter' : [10, 1000],\n",
    "                'n_jobs' : [-1]}\n",
    "\n",
    "#Decision Tree\n",
    "param_grid_DT = {'criterion' : ['gini', 'entropy'],\n",
    "                'splitter' : ['best', 'random'],\n",
    "                'max_depth' : [1, 30],\n",
    "                'min_samples_split' : [2, 50], \n",
    "                'min_samples_leaf' : [1, 50],\n",
    "                'max_features' : [None, 'auto','log2'],\n",
    "                'class_weight' : [None, 'balanced']}\n",
    "\n",
    "#Random Forest\n",
    "param_grid_RF = {'n_estimators': [10,1000],\n",
    "                'criterion' : ['gini', 'entropy'],\n",
    "                'max_depth' : [1, 30],\n",
    "                'min_samples_split' : [2, 50], \n",
    "                'min_samples_leaf' : [1, 50],\n",
    "                'max_features' : [None, 'auto','log2'],\n",
    "                'bootstrap' : [True],\n",
    "                'oob_score' : [False, True],\n",
    "                'n_jobs' : [-1],\n",
    "                'class_weight' : [None, 'balanced']}\n",
    "\n",
    "#XG Boost - note: class weight balance is included as option internally\n",
    "param_grid_XGB = {'booster': ['gbtree'],\n",
    "                'objective' : ['binary:logistic'],\n",
    "                'verbosity' : [0],\n",
    "                'reg_lambda' : [1e-8, 1.0],\n",
    "                'alpha' : [1e-8, 1.0],\n",
    "                'eta' : [1e-8, 1.0],\n",
    "                'gamma' : [1e-8, 1.0],\n",
    "                'max_depth' : [1, 30],\n",
    "                'grow_policy' : ['depthwise', 'lossguide'],\n",
    "                'n_estimators': [10,1000],\n",
    "                'min_samples_split' : [2, 50], \n",
    "                'min_samples_leaf' : [1, 50],\n",
    "                'subsample' : [0.5, 1.0],\n",
    "                'min_child_weight': [0.1, 10],\n",
    "                'colsample_bytree': [0.1, 1.0]}\n",
    "\n",
    "#LG Boost - note: class weight balance is included as option internally\n",
    "param_grid_LGB = {'objective' : ['binary'],\n",
    "                'metric': ['binary_logloss'],\n",
    "                'verbosity' : [-1],\n",
    "                'boosting_type': ['gbdt'],\n",
    "                'num_leaves': [2,256],\n",
    "                'max_depth' : [1, 30],\n",
    "                'lambda_l1': [1e-8, 10.0],\n",
    "                'lambda_l2': [1e-8, 10.0],\n",
    "                'feature_fraction': [0.4, 1.0],\n",
    "                'bagging_fraction': [0.4, 1.0],\n",
    "                'bagging_freq': [1,7],\n",
    "                'min_child_samples': [5,100],\n",
    "                'n_estimators': [10,1000]}\n",
    "\n",
    "#SVM\n",
    "param_grid_SVM = {'kernel' : ['linear','poly','rbf'],\n",
    "                'C': [0.1, 1000],\n",
    "                'gamma': ['scale'],\n",
    "                'degree': [1,6],\n",
    "                'probability' : [True],\n",
    "                'class_weight' : [None, 'balanced']}\n",
    "        \n",
    "#ANN\n",
    "param_grid_ANN = {'n_layers' : [1,3],\n",
    "                'layer_size' : [1,100],\n",
    "                'activation': ['identity','logistic', 'tanh', 'relu'],\n",
    "                'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "                'momentum': [.1,.9],\n",
    "                'solver': ['sgd', 'adam'],\n",
    "                'batch_size': ['auto'],\n",
    "                'alpha': [0.0001, 0.05],\n",
    "                'max_iter' : [200]}\n",
    "\n",
    "param_grid = {} #stores all user selected algorithm hyperparameters\n",
    "\n",
    "for algorithm in algorithmsToRun: #['logistic_regression','decision_tree', 'random_forest', 'naive_bayes', 'xg_boost','SVM','ANN'] \n",
    "    if algorithm == 'logistic_regression':\n",
    "        param_grid['logistic_regression'] = param_grid_LR\n",
    "    elif algorithm == 'decision_tree':\n",
    "        param_grid['decision_tree'] = param_grid_DT\n",
    "    elif algorithm == 'random_forest':\n",
    "        param_grid['random_forest'] = param_grid_RF\n",
    "    elif algorithm == 'naive_bayes':\n",
    "        pass #no hyperparameters\n",
    "    elif algorithm == 'XGB':\n",
    "        param_grid['XGB'] = param_grid_XGB\n",
    "    elif algorithm == 'LGB':\n",
    "        param_grid['LGB'] = param_grid_LGB\n",
    "    elif algorithm == 'SVM':\n",
    "        param_grid['SVM'] = param_grid_SVM\n",
    "    elif algorithm == 'ANN':\n",
    "        param_grid['ANN'] = param_grid_ANN\n",
    "    elif algorithm =='LCS':\n",
    "        pass #parameters are set directly\n",
    "    elif algorithm =='LCS_QRF':\n",
    "        pass #parameters are set directly\n",
    "    else:\n",
    "        print(\"Error: Algorithm not found!\")\n",
    "\n",
    "#LCS Hyperparameters (not set up for parameter sweep - uses specified and default hyperparameters)\n",
    "lcs_path = 'exstracs_2.0.2.1_noclassmutate_lynch/exstracs_main.py' #LCS file to call\n",
    "iterations = 20000\n",
    "popsize = 1000\n",
    "lcs_alg = 'ExSTraCS_2.0.2.1'\n",
    "\n",
    "#Visualization ---------------------------------------------------------------------------------------------------\n",
    "focus_metric = 'Balanced Accuracy'\n",
    "\n",
    "#Statistical Significance-----------------------------------------------------------------------------------------\n",
    "sig_cutoff = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Notebook/Folder Initialization\n",
    "For simplicity it is recommended that this not be altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Names (no need to change)\n",
    "output_folder = 'results'\n",
    "cv_data_folder = 'cv_datasets'\n",
    "ua_folder = 'univariate'\n",
    "model_folder = 'models'\n",
    "lcs_folder = 'lcs'\n",
    "\n",
    "#Working directory path automatically detected\n",
    "wd_path = os.getcwd()\n",
    "wd_path = wd_path.replace('\\\\','/')\n",
    "wd_path = wd_path+'/'\n",
    "print(\"Working Directory: \"+wd_path)\n",
    "\n",
    "#Save data name without extension for later file labeling\n",
    "data_name = target_data.split('.')[0] \n",
    "\n",
    "#Create output folder if it doesn't already exist\n",
    "if not os.path.exists(wd_path+(output_folder)):\n",
    "    os.mkdir(wd_path+(output_folder))\n",
    "\n",
    "#Create CV dataset folder if it doesn't already exist\n",
    "if not os.path.exists(wd_path+(cv_data_folder)):\n",
    "    os.mkdir(wd_path+(cv_data_folder))\n",
    "\n",
    "#Create univariate analysis folder if it doesn't already exist\n",
    "if not os.path.exists(wd_path+(ua_folder)):\n",
    "    os.mkdir(wd_path+(ua_folder))\n",
    "    \n",
    "#Create model pickle folder if it doesn't already exist\n",
    "if not os.path.exists(wd_path+(model_folder)):\n",
    "    os.mkdir(wd_path+(model_folder))\n",
    "    \n",
    "#Create LCS output folder if it doesn't already exist\n",
    "if not os.path.exists(wd_path+(lcs_folder)):\n",
    "    os.mkdir(wd_path+(lcs_folder))\n",
    "\n",
    "data_changed = False #Keeps track of whether cleaning changed original dataset (leave this set to False)\n",
    "\n",
    "run_time_dict = {}\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load Dataset\n",
    "This code assumes that the data is being loaded as a tab delimited '.txt' file with columns as features/outcome, and rows as instances. It is also assumed that the target file includes a header and that missing values are indicated with 'NA' or an empty cell. This command can be updated depending on users dataset format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_csv(target_data, na_values='NA', sep = \"\\t\") #Data load command for tab-delimited .txt file\n",
    "#td = pd.read_csv(target_data, na_values='NA', sep = \",\") #Data load command for comma-separated .csv file\n",
    "#td = pd.read_excel(target_data) #Data load command for excel file\n",
    "\n",
    "td.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging data simplification (This code should be commented out during normal run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#td = td.sample(frac=0.5, replace=False, random_state=randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initial Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows and columns\n",
    "num_rows = td.shape[0]\n",
    "print('Dataset contains '+str(num_rows)+' rows.')\n",
    "num_cols = td.shape[1]\n",
    "print('Dataset contains '+str(num_cols)+' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Missingness in Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(td.columns):\n",
    "    na_percent = np.sum(td[c].isnull())/len(td)*100\n",
    "    if na_percent > 0:\n",
    "        print(c,'\\t\\t% Missing(N/A) = ',np.sum(td[c].isnull())/len(td)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = td.isnull().sum()\n",
    "#Plot a histogram of the missingness observed over all features in the dataset\n",
    "ax = missing_count.hist(bins=num_rows,figsize=(12,4))\n",
    "ax.set_xlabel(\"Missing Value Counts\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Histogram of Missing Value Counts In Feature Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove instances (rows) with missing outcome values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training instances in original dataset = \",num_rows)\n",
    "td = td.dropna(axis=0,how='any',subset=[outcomeLabel])\n",
    "print(\"Number of training instances after removing instances with missing endpoint = \",len(td))\n",
    "diff = num_rows-len(td)\n",
    "print(\"Number of instances removed = \", diff)\n",
    "num_rows = len(td) #reassign value (if changed)\n",
    "if diff > 0:\n",
    "    data_changed = True\n",
    "td.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#It's critical to reset the index after dropping rows to work with data frame later with pd.concat of separate columns\n",
    "td = td.reset_index(drop=True)\n",
    "td.shape\n",
    "#Ensure Class variable is cast as an int\n",
    "td[outcomeLabel] = td[outcomeLabel].astype(dtype='int64')\n",
    "td.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm no missing values in outcome variable\n",
    "na_percent = np.sum(td[outcomeLabel].isnull())/len(td[outcomeLabel])*100\n",
    "if na_percent > 0:\n",
    "    print(c,'\\t\\t% Missing(N/A) = ',np.sum(td[outcomeLabel].isnull())/len(td)*100)\n",
    "else:\n",
    "    print('No missing values found in: '+ str(outcomeLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns to be ignored in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of columns in original dataset = \", num_cols)\n",
    "td = td.drop(ignore_columns, axis=1)\n",
    "print(\"Number of columns after removing ignored columns = \", len(td.columns))\n",
    "diff = num_cols-len(td.columns)\n",
    "print(\"Number of columns removed = \", diff)\n",
    "num_cols = len(td.columns)\n",
    "if diff > 0:\n",
    "    data_changed = True\n",
    "td.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess class imbalance (assuming discrete outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of each class\")\n",
    "print(td[outcomeLabel].value_counts())\n",
    "td[outcomeLabel].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Counts (Checking for Imbalance)')\n",
    "td.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify categorical variables in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if autoDetectCategorical:\n",
    "    #Separate independent variables from the rest\n",
    "    if instLabel == None or instLabel == 'None':\n",
    "        x_data = td.drop([outcomeLabel], axis=1)\n",
    "    else:\n",
    "        x_data = td.drop([outcomeLabel,instLabel], axis=1)\n",
    "\n",
    "    categorical_variables = identifyCategoricalFeatures(x_data,categoricalCutoff)\n",
    "    #print(categorical_variables)\n",
    "    td.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example Code for other potential data cleaning/processing\n",
    "- Not applied in this example script, but available here as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Rows/Instances with any missing values\n",
    "- An extreme alternative to imputation when missing values must be eliminated from the dataset prior to machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_rows = td.shape[0]\n",
    "#print(\"Number of training instances in original dataset = \",num_rows)\n",
    "#td = td.dropna()\n",
    "#print(\"Number of training instances after removing instances with any missing values = \",td.shape[0])\n",
    "#diff = num_rows-td.shape[0]\n",
    "#if diff > 0:\n",
    "#    data_changed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Columns with any missing values\n",
    "- An extreme alternative to imputation when missing values must be eliminated from the dataset prior to machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_cols = td.shape[1]\n",
    "#print(\"Number of training instances in original dataset = \",num_cols)\n",
    "#td = df.dropna(axis='columns')\n",
    "#print(\"Number of training instances after removing instances with any missing values = \",td.shape[1])\n",
    "#diff = num_cols-td.shape[1]\n",
    "#if diff > 0:\n",
    "#    data_changed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding of categorical variables\n",
    "- Ensures that categorical variables are not treated in an ordinal manner by ML algorithms\n",
    "- The code below is an example of how to convert a single column to one hot encoding, and is not meant to be run as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example below assumes a data frame 'df' with a column called 'B'\n",
    "#one_hot = pd.get_dummies(df['B'])\n",
    "#df = df.drop('B',axis = 1)\n",
    "#df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other possible cleaning steps:\n",
    "- encode text values as numerics\n",
    "- oversample or undersample to reduce class imbalance\n",
    "- drop additional rows based on assigned cutoffs, etc\n",
    "- cast variable types as nominal or ordinal\n",
    "- construct missingness features with MissingIndicator: https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved for later, determines if imputation attempted.\n",
    "isMissingData = False\n",
    "if td.isnull().values.any():\n",
    "    isMissingData = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned dataset if it has been modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_changed:\n",
    "    print('Saving cleaned data.')\n",
    "    td.shape\n",
    "    data_name = data_name +'_clean'\n",
    "    td.to_csv(data_name+'.txt', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Further Exploratory/Descriptive Analysis\n",
    "- Visualize feature correlations in data\n",
    "- Appropriate univariate analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCorrelation:\n",
    "    #Drop outcome column for correlation analysis\n",
    "    td_cor = td.drop([outcomeLabel], axis=1)\n",
    "    \n",
    "    #Compute correlation between the outcome and each feature , excluding NA/null values.\n",
    "    corrmat = td_cor.corr(method='pearson') \n",
    "    f, ax = plt.subplots(figsize = (40,20)) # Note: need large size\n",
    "    sns.heatmap(corrmat, vmax =1.0,square = True) # Set max value and square-shaped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis and Plots\n",
    "Below we have encoded a method to automatically select an appropriate plot and univariate association test between a single feature and the target outcome in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to analyze the univariate relationship between the outcome and each feature\n",
    "def test_selector(featureName, outcomeLabel, td, categorical_variables):\n",
    "    p_val = 0\n",
    "    #Feature and Outcome are discrete/categorical/binary\n",
    "    if featureName in categorical_variables:  \n",
    "        #Calculate Contingency Table - Counts\n",
    "        table = pd.crosstab(td[featureName], td[outcomeLabel])\n",
    "        \n",
    "        #Univariate association test (Chi Square Test of Independence - Non-parametric)\n",
    "        c, p, dof, expected = scs.chi2_contingency(table)\n",
    "        p_val = p\n",
    "        \n",
    "    #Feature is continuous and Outcome is discrete/categorical/binary\n",
    "    else: \n",
    "        #Univariate association test (Mann-Whitney Test - Non-parametric)\n",
    "        c, p = scs.mannwhitneyu(x=td[featureName].loc[td[outcomeLabel] == 0],y=td[featureName].loc[td[outcomeLabel] == 1])\n",
    "        p_val = p\n",
    "        \n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function select appropriate visualizations to examine univariate relationships between each feature and outcome\n",
    "\n",
    "def graph_selector(featureName, outcomeLabel, td, categorical_variables):\n",
    "    #Feature and Outcome are discrete/categorical/binary\n",
    "    if featureName in categorical_variables:  \n",
    "        #Generate contingency table count bar plot. ------------------------------------------------------------------------\n",
    "        #Calculate Contingency Table - Counts\n",
    "        table = pd.crosstab(td[featureName], td[outcomeLabel])\n",
    "        geom_bar_data = pd.DataFrame(table)\n",
    "        mygraph = geom_bar_data.plot(kind='bar')\n",
    "        plt.ylabel('Count')\n",
    "        new_feature_name = featureName.replace(\" \",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"*\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"/\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        plt.savefig((wd_path+ua_folder+'/'+'UA_Barplot_' + str(new_feature_name)+'_'+outcomeLabel+'_'+data_name), bbox_inches = \"tight\",format='png')\n",
    "        plt.show()\n",
    "        \n",
    "    #Feature is continuous and Outcome is discrete/categorical/binary    \n",
    "    else: \n",
    "        #Generate boxplot-----------------------------------------------------------------------------------------------------\n",
    "        mygraph = td.boxplot(column=featureName,by=outcomeLabel)\n",
    "        plt.ylabel(featureName)\n",
    "        plt.title('')\n",
    "        new_feature_name = featureName.replace(\" \",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"*\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"/\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        plt.savefig((wd_path+ua_folder+'/'+'UA_Boxplot_' + str(new_feature_name)+'_'+outcomeLabel+'_'+data_name), bbox_inches = \"tight\",format='png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doUA:\n",
    "    p_val_dict = {}\n",
    "    # run appropriate univariate association test for each independent variable\n",
    "    for each in td:\n",
    "        if each != outcomeLabel and each != instLabel: \n",
    "            p_val_dict[each] = test_selector(each, outcomeLabel, td, categorical_variables)\n",
    "    \n",
    "    num_features = td.shape[1] - 1\n",
    "    \n",
    "    sorted_p_list = sorted(p_val_dict.items(), key = lambda item:item[1])\n",
    "    \n",
    "    #Save all p-values to file\n",
    "    pval_df = pd.DataFrame.from_dict(p_val_dict, orient='index')\n",
    "    filepath = wd_path+ua_folder+'/'+'UA_Significance_'+data_name+'.csv'\n",
    "    pval_df.to_csv(filepath, header=['p-value'], index=True)  \n",
    "    \n",
    "    min_num = min(topResults,num_features)\n",
    "    sorted_p_list = sorted_p_list[: min_num]\n",
    "    print('Plotting top significant '+ str(min_num) + ' features.')\n",
    "    \n",
    "    # summarize significant values of selected number of features\n",
    "    print('###################################################')\n",
    "    print('Significant Univariate Associations:')\n",
    "    for each in sorted_p_list[:min_num]:\n",
    "        print(each[0]+\": (p-val = \"+str(each[1]) +\")\")\n",
    "\n",
    "    #generate appropriate descriptive plot for selected number of features\n",
    "    for i in sorted_p_list:\n",
    "        for j in td:\n",
    "            if j == i[0]:# if the feature name is within the selected feature list\n",
    "                graph_selector(j, outcomeLabel, td, categorical_variables) #plot appropriate figure for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create n-fold Cross Validation (CV) Datasets \n",
    "- i.e. Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a header list of independant feature names\n",
    "header = td.columns.values.tolist()\n",
    "header.remove(outcomeLabel)\n",
    "if instLabel != None and instLabel != 'None':\n",
    "    header.remove(instLabel)\n",
    "\n",
    "#Perform CV partitioning (get back list of training and testing partition dataframes)\n",
    "train_dfs, test_dfs = cv_partitioner(td, cv_partitions, partition_method, outcomeLabel, categoricalOutcome, matchName, randomSeed)\n",
    "\n",
    "if partition_method == 'M':\n",
    "    header.remove(matchName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Transform data - Standard Scaling\n",
    "- Features are independently scaled to allow them to be more comparable in downstream logistic regression modeling, etc\n",
    "- Scaling is learned on the training data and saved with pickel so that the same scaling may be applied to any future testing data\n",
    "- Completed within each training and testing partition independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaleData:\n",
    "    print('Scaling Data')\n",
    "    data_name = data_name +'_scale'\n",
    "    name_path = wd_path+model_folder+'/'+'Scalar_' + data_name+'_'\n",
    "    train_dfs, test_dfs = dataScaling(train_dfs, test_dfs, outcomeLabel, instLabel, name_path, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Impute Missing Data Values\n",
    "- Missing data values in categorical features are imputed with simple 'mode' imputation\n",
    "- Remaining missing data values are imputed using Iterative Imputer (i.e. MICE) \n",
    "- Completed within each training and testing partition independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if imputeData and isMissingData:\n",
    "    print('Imputing Data')\n",
    "    data_name = data_name + '_imp'\n",
    "    train_dfs, test_dfs = imputeCVData(outcomeLabel, instLabel, categorical_variables, header, train_dfs, test_dfs, randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm Missing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Datasets\")\n",
    "for each in train_dfs:\n",
    "    each.isnull().sum().sum()\n",
    "\n",
    "print(\"Testing Datasets\")\n",
    "for each in test_dfs:\n",
    "    each.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training and Testing Datasets as .txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = data_name+'_CV_'+str(partition_method)\n",
    "\n",
    "counter = 0\n",
    "for each in train_dfs:\n",
    "    each.to_csv(cv_data_folder+'/'+data_name+'_'+str(counter)+'_Train.txt', index=None, sep='\\t')\n",
    "    counter += 1\n",
    "    \n",
    "counter = 0\n",
    "for each in test_dfs:\n",
    "    each.to_csv(cv_data_folder+'/'+data_name+'_'+str(counter)+'_Test.txt', index=None, sep='\\t')\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load, Check, and Prepare Saved CV Datasets for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CV Partition Datasets\n",
    "Re-loads training and testing datasets from saved .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to hold training and testing set dataframes.\n",
    "trainList = []\n",
    "testList = []\n",
    "\n",
    "for i in range(cv_partitions):\n",
    "    #Load training partition\n",
    "    trainSet = pd.read_csv(cv_data_folder+'/'+data_name+'_'+str(i)+'_Train.txt', na_values='NA', sep = \"\\t\")\n",
    "    trainList.append(trainSet)\n",
    "    \n",
    "    #Load testing partition\n",
    "    testSet = pd.read_csv(cv_data_folder+'/'+data_name+'_'+str(i)+'_Test.txt', na_values='NA', sep = \"\\t\")\n",
    "    testList.append(testSet)\n",
    "    \n",
    "print('Number of Training Partitions: '+ str(len(trainList)))\n",
    "print('Number of Testing Partitions: '+ str(len(testList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Control Check (Checking one of the partitions)\n",
    "Merge the first training/testing datasets to confirm original dataset is reconstituted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm case/control counts when merging one training and testing dataset\n",
    "classesTrain = trainList[0].apply(lambda x: True if x[outcomeLabel] == 1 else False , axis=1)\n",
    "casesTrain = len(classesTrain[classesTrain == True].index)\n",
    "controlsTrain = len(classesTrain[classesTrain == False].index)\n",
    "\n",
    "classesTest = testList[0].apply(lambda x: True if x[outcomeLabel] == 1 else False , axis=1)\n",
    "casesTest = len(classesTest[classesTest == True].index)\n",
    "controlsTest = len(classesTest[classesTest == False].index)\n",
    "\n",
    "print(\"Number of Cases in Original Dataset: \"+str(casesTrain+casesTest))\n",
    "print(\"Number of Controls in Original Dataset: \"+str(controlsTrain+controlsTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm training and testing set dimensions\n",
    "print(\"Training Set Dimensions: \" + str(trainList[0].shape))\n",
    "print(\"Testing Set Dimensions: \" + str(testList[0].shape))\n",
    "numColumns = trainList[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Data Preprocessing\n",
    "If present, remove instance id's from datasets prior to analysis. Also confirm that training and testing sets have same set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Instance ID from all training and test sets\n",
    "if instLabel != None and instLabel != 'None':\n",
    "    for i in range(cv_partitions):\n",
    "        trainList[i] = trainList[i].drop([instLabel] , axis = 1)\n",
    "        testList[i] = testList[i].drop([instLabel] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainColumnNames = list(trainList[0])\n",
    "testColumnNames = list(testList[0])\n",
    "\n",
    "if not trainColumnNames == testColumnNames:\n",
    "    print('Error: Training columns do not match testing columns!')\n",
    "\n",
    "# As we can see, instLabel is no longer a column name for any of the sets\n",
    "#print(trainColumnNames)\n",
    "#print(testColumnNames)\n",
    "\n",
    "#Create master list of feature names (excluding outcome column)\n",
    "ordered_feature_names = copy.deepcopy(trainColumnNames) #Stores original ordered feature list \n",
    "ordered_feature_names.remove(outcomeLabel)\n",
    "#ordered_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data for Scikit learn (separate features from outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into training and test features and outcomes\n",
    "xTrainList = []\n",
    "yTrainList = []\n",
    "xTestList = []\n",
    "yTestList = []\n",
    "\n",
    "for i in range(cv_partitions):\n",
    "    xTrainList.append(trainList[i].iloc[:, 1:].values)\n",
    "    yTrainList.append(trainList[i].iloc[:, 0].values)\n",
    " \n",
    "    xTestList.append(testList[i].iloc[:, 1:].values)\n",
    "    yTestList.append(testList[i].iloc[:, 0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Importance Evaluation\n",
    "- Completed independently within each training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store indexes of 'interesting' features identified by each algorithm in algorithm labeled dictionary\n",
    "selectedFeatureLists = {}\n",
    "metaScoreDict = {}\n",
    "metaFeatureRanks = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Mutual Information for each CV------------------------------------------------------------------------------\n",
    "algorithm = 'mutual_information'\n",
    "if algorithm in algorithms:\n",
    "    start_time = time.time()\n",
    "    scoreSet = [] #Used to save excel filel with data-ordered scores for each CV partition\n",
    "    featureNameRanks = [] #list of feature name ranks for each CV partition\n",
    "    scoreDictSet = [] #list of feature/score dictionaries (one for each cv partition)\n",
    "    cvKeepList = [] #Used for initial feature selection attempt to keep only features with 'informative' support\n",
    "    \n",
    "    for i in tqdm_notebook(range(cv_partitions), desc='1st loop'):\n",
    "        #Run algorithm and return scores and sorted score dictionary\n",
    "        scores, scoreDict, score_sorted_features = run_mi(xTrainList[i],yTrainList[i], i, data_name, output_folder,randomSeed,ordered_feature_names,algorithm)\n",
    "        scoreSet.append(scores) #store data-ordered scores for this cv partition\n",
    "        scoreDictSet.append(scoreDict)\n",
    "        featureNameRanks.append(score_sorted_features)\n",
    "        \n",
    "        #Add list of feature names that meet the 'informative' cutoff\n",
    "        keepList = []\n",
    "        for each in scoreDict:\n",
    "            if scoreDict[each] > 0:\n",
    "                keepList.append(each)\n",
    "                #keepList.append(ordered_feature_names.index(each))\n",
    "        cvKeepList.append(keepList)\n",
    "        \n",
    "        #Create dictionary of average scores\n",
    "        if i == 0:\n",
    "            scoreSum = copy.deepcopy(scoreDict)\n",
    "        else:\n",
    "            for each in scoreDict:\n",
    "                scoreSum[each] += scoreDict[each]\n",
    "    \n",
    "    #Save/report analysis results\n",
    "    #cv_output_name = data_name+'_clean_imp_CV_'+partition_method\n",
    "    reportAllFS(scoreSet, algorithm, ordered_feature_names,output_folder,data_name) #Save CV scores file for algorithm\n",
    "    reportTopFS(scoreSum, algorithm, cv_partitions,topResults,wd_path,output_folder,data_name)\n",
    "    \n",
    "    #Store info for downstream feature selection\n",
    "    selectedFeatureLists[algorithm] = cvKeepList\n",
    "    metaScoreDict[algorithm] = scoreDictSet\n",
    "    metaFeatureRanks[algorithm] = featureNameRanks\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Mutual Information Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiSURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Run MultiSURF for each CV------------------------------------------------------------------------------\n",
    "algorithm = 'multisurf'\n",
    "if algorithm in algorithms:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    scoreSet = [] #Used to save excel filel with data-ordered scores for each CV partition\n",
    "    scoreDictSet = [] #list of feature/score dictionaries (one for each cv partition)\n",
    "    cvKeepList = [] #Used for initial feature selection attempt to keep only features with 'informative' support\n",
    "    \n",
    "    for i in tqdm_notebook(range(cv_partitions), desc='1st loop'):\n",
    "        #If training set is large MultiSURF will run very slowly - quadradic time complexity with # instances (so use subset)\n",
    "        tempData = pd.concat([pd.DataFrame(yTrainList[i]),pd.DataFrame(xTrainList[i])], axis=1, sort=False)\n",
    "        if len(tempData) < instanceSubset:\n",
    "            dataSample = tempData.sample(n=len(tempData), axis = 0)\n",
    "        else:\n",
    "            dataSample = tempData.sample(n=instanceSubset, axis = 0)\n",
    "\n",
    "        xTrain = dataSample.iloc[:, 1:].values\n",
    "        yTrain = dataSample.iloc[:, 0].values\n",
    "        #Run algorithm and return scores and sorted score dictionary\n",
    "        scores, scoreDict, score_sorted_features = run_multisurf(xTrain,yTrain, i, data_name, output_folder,randomSeed,ordered_feature_names,algorithm)\n",
    "        scoreSet.append(scores) #store data-ordered scores for this cv partition\n",
    "        scoreDictSet.append(scoreDict)\n",
    "        featureNameRanks.append(score_sorted_features)\n",
    "        \n",
    "        #Add list of feature names that meet the 'informative' cutoff\n",
    "        keepList = []\n",
    "        for each in scoreDict:\n",
    "            if scoreDict[each] > 0:\n",
    "                keepList.append(each)\n",
    "                #keepList.append(ordered_feature_names.index(each))\n",
    "        cvKeepList.append(keepList)\n",
    "        \n",
    "        #Create dictionary of average scores\n",
    "        if i == 0:\n",
    "            scoreSum = copy.deepcopy(scoreDict)\n",
    "        else:\n",
    "            for each in scoreDict:\n",
    "                scoreSum[each] += scoreDict[each]\n",
    "    \n",
    "    #Save/report analysis results\n",
    "    reportAllFS(scoreSet, algorithm, ordered_feature_names,output_folder,data_name) #Save CV scores file for algorithm\n",
    "    reportTopFS(scoreSum, algorithm, cv_partitions,topResults,wd_path,output_folder,data_name)\n",
    "    \n",
    "    #Store info for downstream feature selection\n",
    "    selectedFeatureLists[algorithm] = cvKeepList\n",
    "    metaScoreDict[algorithm] = scoreDictSet\n",
    "    metaFeatureRanks[algorithm] = featureNameRanks\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('MultiSURF Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify union of 'interesting' features from different algorithms\n",
    "- Operates no matter how many feature selection methods are being applied\n",
    "- But assumes that all methods have larger scores as being more informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_Selected_List = selectFeatures(algorithms, cv_partitions, selectedFeatureLists, maxFeaturesToKeep,metaFeatureRanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report counts of 'interesting' features identified by each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original feature count: '+str(len(ordered_feature_names)))\n",
    "\n",
    "for each in algorithms:\n",
    "    print(each)\n",
    "    for cv in selectedFeatureLists[each]:\n",
    "        print(len(cv))\n",
    "    \n",
    "print(\"Selected Counts\")\n",
    "for each in cv_Selected_List:\n",
    "    print(len(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Filtered Datasets (Save as new .txt files)\n",
    "- These datasets included only the selected features for each respective partition\n",
    "- Note that the feature order from the original datasets is no longer preserved in these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filterPoorFeatures:\n",
    "    genFilteredDatasets(cv_Selected_List, outcomeLabel, instLabel,cv_partitions,cv_data_folder,data_name)\n",
    "    data_name = data_name +'_FS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Filtered MultiSURF Files (Save as new .txt files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'multisurf' in algorithms: #This method is only meant for working with MultiSURF scores\n",
    "    if filterPoorFeatures:\n",
    "        for i in range(cv_partitions):\n",
    "            tempDict = scoreDictSet[i]\n",
    "\n",
    "            #create new dictionary with only selected features\n",
    "            scoreDict = {}\n",
    "            for j in cv_Selected_List[i]:\n",
    "                scoreDict[j] = tempDict[j]\n",
    "            \n",
    "            filename = output_folder+'/'+'multisurf'+'_'+data_name+'_'+str(i)+'_Train.txt'\n",
    "            sort_save_fs_fi_scores(scoreDict, 'multisurf', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load, Check, and Prepare Filtered CV Datasets for ML\n",
    "- If user opted not to filter datasets then the code below will simply reload the original CV datasets again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CV Partition Datasets (Filtered Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to hold training and testing set dataframes.\n",
    "trainList = []\n",
    "testList = []\n",
    "\n",
    "for i in range(cv_partitions):\n",
    "    #Load training partition\n",
    "    trainset_name = cv_data_folder+'/'+data_name+'_'+str(i)+'_Train.txt'\n",
    "    trainSet = pd.read_csv(trainset_name, na_values='NA', sep = \"\\t\")\n",
    "    trainList.append(trainSet)\n",
    "\n",
    "    #Load testing partition\n",
    "    testset_name = cv_data_folder+'/'+data_name+'_'+str(i)+'_Test.txt'\n",
    "    testSet = pd.read_csv(testset_name, na_values='NA', sep = \"\\t\")\n",
    "    testList.append(testSet)\n",
    "    \n",
    "print('Number of Training Partitions: '+ str(len(trainList)))\n",
    "print('Number of Testing Partitions: '+ str(len(testList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV partition data cleaning (Filtered Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Instance ID from all training and test sets\n",
    "if instLabel != None and instLabel != 'None':\n",
    "    for i in range(cv_partitions):\n",
    "        trainList[i] = trainList[i].drop([instLabel] , axis = 1)\n",
    "        testList[i] = testList[i].drop([instLabel] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ordered_features = [] #Holds the new order of features in the FS CV datasets\n",
    "for i in range(cv_partitions):\n",
    "    cv_ordered_features = list(trainList[i])\n",
    "    cv_ordered_features.remove(outcomeLabel)\n",
    "    #print(cv_ordered_features)\n",
    "    global_ordered_features.append(cv_ordered_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data for Scikit learn (filtered datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into training and test features and outcomes\n",
    "xTrainList = []\n",
    "yTrainList = []\n",
    "xTestList = []\n",
    "yTestList = []\n",
    "\n",
    "for i in range(cv_partitions):\n",
    "    xTrainList.append(trainList[i].iloc[:, 1:].values)\n",
    "    yTrainList.append(trainList[i].iloc[:, 0].values)\n",
    " \n",
    "    xTestList.append(testList[i].iloc[:, 1:].values)\n",
    "    yTestList.append(testList[i].iloc[:, 0].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Machine Learning Modeling\n",
    "ML algorithms are run one after the other below. Within each we conduct a hyperparameter sweep, train a best model and evaluate it indendently within each CV training/testing partition. \n",
    "- All standard classification metrics are computed for the resulting models as well as an ROC curve and precision/recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'logistic_regression'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "\n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Logistic Regression Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'decision_tree'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "    \n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Decision Tree Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'random_forest'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "\n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Random Forest Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'naive_bayes'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "        \n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Naive Bayes Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'XGB'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "\n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('XGBoost Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'LGB'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "\n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "        \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('LGBoost Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'SVM'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "\n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('SVM Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'ANN'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec, FI_ave = eval_Algorithm_FI(algorithm,ordered_feature_names,\n",
    "        xTrainList,yTrainList,xTestList,yTestList,cv_partitions,global_ordered_features,wd_path,output_folder,\n",
    "        data_name,randomSeed,param_grid,model_folder,algColor,hype_cv,n_trials,scoring_metric,timeout)\n",
    "    \n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "\n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('ANN Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfigFile(train_dataset,test_dataset,outpath,external_ek_file,iterations,popsize,configfile,outcomelabel,instancelabel,discthresh):\n",
    "    \"\"\" Construct Configuration File for CV Analysis \"\"\"\n",
    "    configFile = open(configfile,'w')\n",
    "    \n",
    "    #Write to Config File#############################################################################################################\n",
    "    configFile.write('offlineData=1# \\n')\n",
    "    configFile.write('trainFile='+str(train_dataset)+'# \\n')\n",
    "    configFile.write('testFile='+str(test_dataset)+'# \\n')\n",
    "    configFile.write('internalCrossValidation=0# \\n')\n",
    "    \n",
    "    configFile.write('outFileName='+str(outpath)+'# \\n')\n",
    "    configFile.write('randomSeed=1# \\n')\n",
    "    configFile.write('labelInstanceID='+str(instancelabel)+'# \\n')\n",
    "    configFile.write('labelPhenotype='+str(outcomelabel)+'# \\n')\n",
    "    configFile.write('discreteAttributeLimit='+str(discthresh)+'# \\n')\n",
    "    configFile.write('labelMissingData=NA# \\n')\n",
    "    configFile.write('outputSummary=1# \\n')\n",
    "    configFile.write('outputPopulation=1# \\n')\n",
    "    configFile.write('outputAttCoOccur=1# \\n')\n",
    "    configFile.write('outputTestPredictions=1# \\n')  \n",
    "    configFile.write('maxAttsCoOccur=50# \\n')\n",
    "        \n",
    "    configFile.write('trackingFrequency=0# \\n')\n",
    "    configFile.write('learningIterations='+str(iterations)+'# \\n')\n",
    "    \n",
    "    configFile.write('N='+str(popsize)+'# \\n') \n",
    "    configFile.write('nu=1# \\n')\n",
    "    configFile.write('chi=0.8# \\n')\n",
    "    configFile.write('upsilon=0.04# \\n')\n",
    "    configFile.write('theta_GA=25# \\n')\n",
    "    configFile.write('theta_del=20# \\n')   \n",
    "    configFile.write('theta_sub=20# \\n')    \n",
    "    configFile.write('acc_sub=0.99# \\n')    \n",
    "    configFile.write('beta=0.2# \\n')         \n",
    "    configFile.write('delta=0.1# \\n')     \n",
    "    configFile.write('init_fit=0.01# \\n')\n",
    "    configFile.write('fitnessReduction=0.1# \\n')\n",
    "    configFile.write('theta_sel=0.5# \\n')\n",
    "    configFile.write('RSL_Override=0# \\n')\n",
    "\n",
    "    configFile.write('doSubsumption=1# \\n')\n",
    "    configFile.write('selectionMethod=tournament# \\n')\n",
    "    \n",
    "    configFile.write('doAttributeTracking=1# \\n')\n",
    "    configFile.write('doAttributeFeedback=1# \\n')\n",
    "    \n",
    "    configFile.write('useExpertKnowledge=1# \\n')\n",
    "    configFile.write('external_EK_Generation='+str(external_ek_file)+'# \\n')\n",
    "    configFile.write('outEKFileName=None# \\n')\n",
    "    \n",
    "    configFile.write('filterAlgorithm=None# \\n')\n",
    "    configFile.write('turfPercent=0.2# \\n')\n",
    "    configFile.write('reliefNeighbors=10# \\n')\n",
    "    configFile.write('reliefSampleFraction=1# \\n')\n",
    "    configFile.write('onlyEKScores=0# \\n')\n",
    "    \n",
    "    configFile.write('doRuleCompaction=1# \\n')\n",
    "    configFile.write('onlyRC=0# \\n')\n",
    "    configFile.write('ruleCompactionMethod=QRF# \\n')\n",
    "    \n",
    "    configFile.write('doPopulationReboot=0# \\n')\n",
    "    configFile.write('popRebootIteration=0# \\n')\n",
    "    \n",
    "    configFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate LCS Configuration Files (needed to pass all run/hyperparamters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'LCS'\n",
    "if algorithm in algorithmsToRun:\n",
    "    start_time = time.time()\n",
    "    # Generate LCS configuration files (needed to pass all run/hyper parameters to LCS)\n",
    "    for i in tqdm_notebook(range(cv_partitions), desc='1st loop'):\n",
    "        trainFile = wd_path+cv_data_folder+'/'+data_name+'_'+str(i)+'_Train.txt'\n",
    "        testFile = wd_path+cv_data_folder+'/'+data_name+'_'+str(i)+'_Test.txt'\n",
    "        ekFile = wd_path+output_folder+'/'+'multisurf'+'_'+data_name+'_'+str(i)+'_Train.txt'\n",
    "        configfile = wd_path+lcs_folder+'/'+lcs_alg+'_'+data_name+'_'+str(i)+'_Train_ConfigFile.txt'\n",
    "        outFold = wd_path+lcs_folder+'/'\n",
    "\n",
    "        makeConfigFile(trainFile,testFile,outFold,ekFile,iterations,popsize,configfile,outcomeLabel,instLabel,categoricalCutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'LCS'\n",
    "if algorithm in algorithmsToRun:\n",
    "    #Use Python Magic to run Exstracs from command line\n",
    "    for i in tqdm_notebook(range(cv_partitions), desc='1st loop'):\n",
    "        #lcs_path = 'exstracs_2.0.2.1_noclassmutate_lynch'+'/'+'exstracs_main.py'\n",
    "        configFile = wd_path+lcs_folder+'/'+lcs_alg+'_'+data_name+'_'+str(i)+'_Train_ConfigFile.txt'\n",
    "        %run {lcs_path} {configFile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate LCS Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algorithm = 'LCS'\n",
    "if algorithm in algorithmsToRun:\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec = eval_LCS(algorithm,cv_partitions,wd_path,output_folder,lcs_folder,data_name,iterations,algColor)\n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('LCS Run Time: '+ str(elapsed_time))\n",
    "    run_time_dict[algorithm] = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate LCS with QRF Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'LCS_QRF'\n",
    "if algorithm in algorithmsToRun:\n",
    "    algColor = algColors[algorithmsToRun.index(algorithm)]\n",
    "    mean_fpr, mean_tpr, mean_auc, mean_prec, mean_pr_auc, mean_ave_prec = eval_LCS_QRF(algorithm,cv_partitions,wd_path,output_folder,lcs_folder,data_name,iterations,algColor)\n",
    "    result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "    pickle.dump(result_dict, open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate LCS Feature Importance Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'LCS'\n",
    "if algorithm in algorithmsToRun:\n",
    "    FI_all = []\n",
    "    FI_ave = [0]*len(ordered_feature_names) #Holds only the selected feature FI results for each partition\n",
    "    \n",
    "    #Gather all LCS feature importance results (specificity sums)\n",
    "    for i in tqdm_notebook(range(cv_partitions), desc='1st loop'):\n",
    "        scoreDict = {}\n",
    "        fiFile = wd_path+lcs_folder+'/'+'ExSTraCS'+'_'+data_name+'_'+str(i)+'_Train_'+str(iterations)+'_PopStats.txt'\n",
    "        fileObject = open(fiFile, 'r') \n",
    "        \n",
    "        counter = 0\n",
    "        tempList1 = []\n",
    "        tempList2 = []\n",
    "        for line in fileObject:\n",
    "            if counter == 9:\n",
    "                tempList1 = line.strip().split('\\t')\n",
    "            if counter == 10:\n",
    "                tempList2 = line.strip().split('\\t')\n",
    "            counter += 1\n",
    "        fileObject.close()\n",
    "        \n",
    "        for j in range(len(tempList1)):\n",
    "            scoreDict[tempList1[j]] = float(tempList2[j])\n",
    " \n",
    "        #Format feature importance scores as list (takes into account that all features are not in each CV partition)\n",
    "        tempList = []\n",
    "        j = 0\n",
    "        for each in ordered_feature_names:\n",
    "            if each in scoreDict:\n",
    "                FI_ave[j] += float(scoreDict[each])\n",
    "                tempList.append(float(scoreDict[each]))\n",
    "            else:\n",
    "                tempList.append(0)\n",
    "            j += 1\n",
    "            \n",
    "        FI_all.append(tempList)\n",
    "    \n",
    "    \n",
    "    dr = pd.DataFrame(FI_all)\n",
    "    filepath = output_folder+'/'+algorithm+'_FI_'+data_name+'.csv'\n",
    "    dr.to_csv(filepath, header=ordered_feature_names, index=False)  \n",
    "    \n",
    "    #Calculate Averages\n",
    "    for i in range(0,len(FI_ave)):\n",
    "        FI_ave[i] = FI_ave[i]/float(cv_partitions)\n",
    "        \n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Evaluate LCS with QRF Feature Importance Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'LCS_QRF'\n",
    "if algorithm in algorithmsToRun:\n",
    "    FI_all = []\n",
    "    FI_ave = [0]*len(ordered_feature_names) #Holds only the selected feature FI results for each partition\n",
    "    \n",
    "    #Gather all LCS feature importance results (specificity sums)\n",
    "    for i in tqdm_notebook(range(cv_partitions), desc='1st loop'):\n",
    "        scoreDict = {}\n",
    "        fiFile = wd_path+lcs_folder+'/'+'ExSTraCS'+'_'+data_name+'_'+str(i)+'_Train_RC_QRF_'+str(iterations)+'_PopStats.txt'\n",
    "        fileObject = open(fiFile, 'r') \n",
    "        \n",
    "        counter = 0\n",
    "        tempList1 = []\n",
    "        tempList2 = []\n",
    "        for line in fileObject:\n",
    "            if counter == 9:\n",
    "                tempList1 = line.strip().split('\\t')\n",
    "            if counter == 10:\n",
    "                tempList2 = line.strip().split('\\t')\n",
    "            counter += 1\n",
    "        fileObject.close()\n",
    "        \n",
    "        for j in range(len(tempList1)):\n",
    "            scoreDict[tempList1[j]] = float(tempList2[j])\n",
    " \n",
    "        #Format feature importance scores as list (takes into account that all features are not in each CV partition)\n",
    "        tempList = []\n",
    "        j = 0\n",
    "        for each in ordered_feature_names:\n",
    "            if each in scoreDict:\n",
    "                FI_ave[j] += float(scoreDict[each])\n",
    "                tempList.append(float(scoreDict[each]))\n",
    "            else:\n",
    "                tempList.append(0)\n",
    "            j += 1\n",
    "            \n",
    "        FI_all.append(tempList)\n",
    "    \n",
    "    \n",
    "    dr = pd.DataFrame(FI_all)\n",
    "    filepath = output_folder+'/'+algorithm+'_FI_'+data_name+'.csv'\n",
    "    dr.to_csv(filepath, header=ordered_feature_names, index=False)  \n",
    "    \n",
    "    #Calculate Averages\n",
    "    for i in range(0,len(FI_ave)):\n",
    "        FI_ave[i] = FI_ave[i]/float(cv_partitions)\n",
    "        \n",
    "    #Sort averages (decreasing order and print top 'n' and plot top 'n'\n",
    "    names_scores = {'Names':ordered_feature_names, 'Scores':FI_ave} \n",
    "    ns = pd.DataFrame(names_scores)\n",
    "    ns = ns.sort_values(by='Scores',ascending = False)\n",
    "\n",
    "    #Select top 'n' to report and plot\n",
    "    print(\"\\n---Feature Importances---\\n\")\n",
    "    ns.head(topResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Review result table (for global ROC and PRC figure building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pickle files of results\n",
    "result_table = []\n",
    "\n",
    "for algorithm in algorithmsToRun:\n",
    "    tempDict = pickle.load(open(wd_path+model_folder+'/Results_'+algorithm+'.sav', 'rb'))\n",
    "    result_table.append(tempDict)\n",
    "    \n",
    "result_table = pd.DataFrame.from_dict(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table\n",
    "result_table.set_index('algorithm', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ROC Plot Summarizing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-algorithm ROC plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "count = 0\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], color = algColors[count],\n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    count += 1\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('Comparing Algorithms: Testing Data with CV', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='best')\n",
    "plt.savefig((wd_path+output_folder+'/'+'Compare_ROC_' + data_name), bbox_inches = \"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PRC Plot Summarizing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-algorithm PRC plot ['algorithm','fpr','tpr','auc','prec','pr_auc','ave_prec'])\n",
    "#result_table.set_index('algorithm', inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "count = 0\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['prec'], color = algColors[count],\n",
    "             label=\"{}, AUC={:.3f}, APS={:.3f}\".format(i, result_table.loc[i]['pr_auc'],result_table.loc[i]['ave_prec']))\n",
    "    count += 1\n",
    "noskill = len(yTestList[0][yTestList[0]==1]) / len(yTestList[0]) #Fraction of cases\n",
    "plt.plot([0,1], [noskill,noskill], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"Recall\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"Precision\", fontsize=15)\n",
    "\n",
    "plt.title('Comparing Algorithms: Testing Data with CV', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='best')\n",
    "plt.savefig((wd_path+output_folder+'/'+'Compare_PRC_' + data_name), bbox_inches = \"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summarize Algorithm Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_csv(wd_path+output_folder+'/'+algorithmsToRun[0]+'_Metrics_'+data_name+'.csv')\n",
    "metrics = list(td.columns.values.tolist()) \n",
    "\n",
    "metric_summary = pd.DataFrame(index=metrics,columns=algorithmsToRun)\n",
    "master_list = []\n",
    "\n",
    "for algorithm in algorithmsToRun:\n",
    "    #Open file\n",
    "    td = pd.read_csv(wd_path+output_folder+'/'+algorithm+'_Metrics_'+data_name+'.csv')\n",
    "    master_list.append(td)\n",
    "    for metric in metrics:\n",
    "        ave = td[metric].mean()\n",
    "        sd = td[metric].std()\n",
    "        metric_summary.at[metric, algorithm] = str(round(ave,4))+' ('+str(round(sd,4))+')'\n",
    "    \n",
    "metric_summary.to_csv(wd_path+output_folder+'/'+'Summary_Algorithm_Metrics_'+data_name+'.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Performance Metric Boxplots Comparing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For each metric\n",
    "for metric in metrics:\n",
    "    i = 0\n",
    "    tempList = []\n",
    "    for algorithm in algorithmsToRun:\n",
    "        temp_td = master_list[i]\n",
    "        tempList.append(temp_td[metric].tolist())\n",
    "        i += 1\n",
    "        \n",
    "    td = pd.DataFrame(tempList)\n",
    "    td = td.transpose()\n",
    "    td.columns = algorithmsToRun\n",
    "        \n",
    "    boxplot = td.boxplot(column=algorithmsToRun,rot=45)\n",
    "    plt.title('Comparing Algorithm '+ str(metric))\n",
    "    plt.ylabel(str(metric))\n",
    "    plt.xlabel('ML Algorithm')\n",
    "    plt.savefig((wd_path+output_folder+'/'+'Compare_'+str(metric)+'_Boxplot_'+data_name), bbox_inches = \"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Statistical Comparisons\n",
    "- Kruskal Wallis AOV comparing ML algorithms\n",
    "- Mann-Whitney U test - Pairwise algorithm differences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kruskal Wallis Test - Algorithm comparisons for each metric\n",
    "- Are any algorithms performing statistically better or worse? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann-Whiney Test - Pairwise Post-Hoc Analysis \n",
    "- For any metric where kruskal wallis test was significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(algorithmsToRun) > 1:\n",
    "    label = ['statistic','pvalue','sig']\n",
    "    kruskal_summary = pd.DataFrame(index=metrics,columns=label)\n",
    "    for metric in metrics:\n",
    "        tempArray = []\n",
    "        for i in range(len(algorithmsToRun)):\n",
    "            tempArray.append(master_list[i][metric])\n",
    "        result = stats.kruskal(*tempArray)\n",
    "        #result = stats.kruskal(master_list[0][metric],master_list[1][metric],master_list[2][metric],master_list[3][metric],master_list[4][metric],master_list[5][metric],master_list[6][metric]) \n",
    "        kruskal_summary.at[metric, 'statistic'] = str(round(result[0],6))\n",
    "        kruskal_summary.at[metric, 'pvalue'] = str(round(result[1],6))\n",
    "        if result[1] < sig_cutoff:\n",
    "            kruskal_summary.at[metric, 'sig'] = str('*')\n",
    "        else:\n",
    "            kruskal_summary.at[metric, 'sig'] = str('')\n",
    "\n",
    "    kruskal_summary\n",
    "\n",
    "    kruskal_summary.to_csv(wd_path+output_folder+'/'+'Summary_Algorithm_KruskalWallis_'+data_name+'.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(algorithmsToRun) > 1:\n",
    "    algCount = len(algorithmsToRun)\n",
    "    for metric in metrics:\n",
    "        if kruskal_summary['sig'][metric] == '*':\n",
    "            mann_stats = []\n",
    "            #Balanced Accuracy\n",
    "            for i in range(0,algCount-1):\n",
    "                for j in range(i+1,algCount):\n",
    "                    set1 = master_list[i][metric]\n",
    "                    set2 = master_list[j][metric]\n",
    "\n",
    "                    #print('Mann-WhitneyU: '+str(algorithmsToRun[i])+' vs. '+str(algorithmsToRun[j]))\n",
    "                    report = stats.mannwhitneyu(set1,set2)\n",
    "                    #report\n",
    "                    tempstats = [algorithmsToRun[i], algorithmsToRun[j], report[0], report[1], '']\n",
    "                    if report[1] < sig_cutoff:\n",
    "                        tempstats[4] = '*'\n",
    "                    mann_stats.append(tempstats)\n",
    "\n",
    "            mann_stats_df = pd.DataFrame(mann_stats)\n",
    "            mann_stats_df.columns = ['Algorithm 1', 'Algorithm 2', 'statistic', 'p-value', 'sig']\n",
    "            filepath = wd_path+output_folder+'/'+'MannWhitney_'+str(metric)+'_'+data_name+'.csv'\n",
    "            mann_stats_df.to_csv(filepath, index = False) \n",
    "            mann_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualize Feature Importance Results Across Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify dataset specific paths/names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figx = 24\n",
    "figy = 12\n",
    "    \n",
    "#Feature names to use in the final publication figures\n",
    "df = pd.read_csv(wd_path+output_folder+'/'+algorithmsToRun[0]+'_FI_'+data_name+'.csv')\n",
    "\n",
    "printfeatureNames = list(df.columns)\n",
    "print(printfeatureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature importance and balanced accuracy results\n",
    "- Do this generically so that future algorithms can be added here\n",
    "- Identify a feature list for each algorithm (only features with average scores over zero)\n",
    "- Identify union of these feature lists - master feature list to be included in bar chart\n",
    "- Have a max_features to visualize - use ranking to reduce above list as needed to top n features only.\n",
    "- Add ability to load LCS or other external algorithm to these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm feature importance dataframe list (used to generate FI boxplots for each algorithm)\n",
    "fi_df_list = []\n",
    "#algorithm feature importance averages list (used to generate composite FI barplots)\n",
    "fi_ave_list = []\n",
    "#algorithm focus metric averages list (used in weighted FI viz)\n",
    "ave_metric_list = []\n",
    "#list of pre-feature selection features as they appear in FI reports for each algorithm\n",
    "all_feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in algorithmsToRun:\n",
    "    #Get relevant feature importance info\n",
    "    temp_df = pd.read_csv(wd_path+output_folder+'/'+each+'_FI_'+data_name+'.csv')\n",
    "    if each == algorithmsToRun[0]: #Should be same for all algorithm files (i.e. all original features in standard CV dataset order)\n",
    "        all_feature_list = temp_df.columns.tolist()\n",
    "    fi_df_list.append(temp_df) \n",
    "    fi_ave_list.append(temp_df.mean().tolist())\n",
    "    \n",
    "    #Get relevant metric info\n",
    "    tmp_df = pd.read_csv(wd_path+output_folder+'/'+each+'_Metrics_'+data_name+'.csv')\n",
    "    ave_metric_list.append(tmp_df[focus_metric].mean())\n",
    "ave_metric_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize average scores (Range 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize average scores so all values range between 0 and 1 (important for combining FI's)\n",
    "fi_ave_norm_list = []\n",
    "for each in fi_ave_list: #each algorithm   \n",
    "    normList = []\n",
    "    for i in range(len(each)):\n",
    "        if each[i] <= 0:\n",
    "            normList.append(0)\n",
    "        else:\n",
    "            normList.append((each[i])/(max(each)))\n",
    "    fi_ave_norm_list.append(normList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify features with non-zero averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each algorithm identify all non-zero features\n",
    "alg_non_zero_FI_list = []\n",
    "for each in fi_ave_list: #each algorithm\n",
    "    \n",
    "    temp_non_zero_list = []\n",
    "    for i in range(len(each)): #each feature\n",
    "        if each[i] > 0.0:\n",
    "            temp_non_zero_list.append(all_feature_list[i])\n",
    "    \n",
    "    alg_non_zero_FI_list.append(temp_non_zero_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify union of features with non-zero averages over all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify a union list over all algorithms of features with non-zero FI (These are candidates for vizualization)\n",
    "non_zero_union_features = alg_non_zero_FI_list[0] #grab first algorithm's list\n",
    "\n",
    "for j in range(1,len(algorithmsToRun)):\n",
    "    #print(j)\n",
    "    non_zero_union_features = list(set(non_zero_union_features) | set(alg_non_zero_FI_list[j]))\n",
    "\n",
    "#print(non_zero_union_features)\n",
    "\n",
    "non_zero_union_indexes = []\n",
    "for i in non_zero_union_features:\n",
    "    non_zero_union_indexes.append(all_feature_list.index(i))\n",
    "non_zero_union_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify list of top features over all algorithms to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If remaining non-zero feature list is still larger than we want to visualize, pick only top features for visualization\n",
    "featuresToViz = None\n",
    "if len(non_zero_union_features) > topResults:\n",
    "    #Identify a top set of feature values\n",
    "    scoreSumDict = {}\n",
    "    i = 0\n",
    "    for each in non_zero_union_features: #for each non-zero feature\n",
    "        for j in range(len(algorithmsToRun)): #for each algorithm\n",
    "            #grab target score from each algorithm\n",
    "            score = fi_ave_norm_list[j][non_zero_union_indexes[i]]\n",
    "            #multiply score by algorithm performance weight\n",
    "            score = score*ave_metric_list[j]\n",
    "            if not each in scoreSumDict:\n",
    "                scoreSumDict[each] = score\n",
    "            else:\n",
    "                scoreSumDict[each] += score\n",
    "        i += 1\n",
    "        \n",
    "    for each in scoreSumDict:\n",
    "        scoreSumDict[each] = scoreSumDict[each]/ len(algorithmsToRun)\n",
    "    #print(scoreSumDict)\n",
    "\n",
    "    #Rank and select top features to visualize\n",
    "\n",
    "    #Sort features by decreasing score\n",
    "    scoreSumDict_features = sorted(scoreSumDict, key=lambda x: scoreSumDict[x], reverse=True)\n",
    "    #scoreSumDict_features\n",
    "\n",
    "    featuresToViz = scoreSumDict_features[0:topResults]\n",
    "    #featuresToViz\n",
    "else:\n",
    "    featuresToViz = non_zero_union_features #Ranked feature name order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate individual feature importance boxplots for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for df in fi_df_list:\n",
    "    fig = plt.figure(figsize=(15,4))\n",
    "    boxplot = df.boxplot(rot=90)\n",
    "    plt.title(algorithmsToRun[counter])\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.xlabel('Features')\n",
    "    plt.xticks(np.arange(1,len(printfeatureNames)+1), printfeatureNames,rotation='vertical')\n",
    "    plt.savefig((wd_path+output_folder+'/'+algorithmsToRun[counter]+'_boxplot_' + data_name), bbox_inches = \"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Normalized dataframes with feature viz subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresToViz\n",
    "\n",
    "feature_indexToViz = []\n",
    "for i in featuresToViz:\n",
    "    feature_indexToViz.append(all_feature_list.index(i))\n",
    "feature_indexToViz\n",
    "\n",
    "#Preserve features in original dataset order for consistency\n",
    "top_fi_ave_norm_list = []\n",
    "for i in range(len(algorithmsToRun)):\n",
    "    tempList = []\n",
    "    for j in range(len(fi_ave_norm_list[i])):\n",
    "        if j in feature_indexToViz:\n",
    "            tempList.append(fi_ave_norm_list[i][j])\n",
    "    top_fi_ave_norm_list.append(tempList)\n",
    "\n",
    "#Create feature name list in propper order\n",
    "all_feature_listToViz = []\n",
    "for j in (all_feature_list):\n",
    "    if j in featuresToViz:\n",
    "        all_feature_listToViz.append(j)\n",
    "all_feature_listToViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_FI_plot(fi_list,algorithmsToRun,algColors,all_feature_listToViz,figName):\n",
    "    \n",
    "    # y-axis in bold\n",
    "    rc('font', weight='bold', size=16)\n",
    "    \n",
    "    # The position of the bars on the x-axis\n",
    "    r = all_feature_listToViz\n",
    "    barWidth = 0.75\n",
    "    plt.figure(figsize=(figx,figy))\n",
    "            \n",
    "    lines = None\n",
    "    if len(algorithmsToRun) == 1:\n",
    "        print(\"Plotting with 1 algorithm.\")\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0])\n",
    "        \n",
    "    elif len(algorithmsToRun) == 2:\n",
    "        print(\"Plotting with 2 algorithms.\")\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0])\n",
    "        \n",
    "    elif len(algorithmsToRun) == 3:\n",
    "        print(\"Plotting with 3 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0],p3[0])\n",
    "        \n",
    "    elif len(algorithmsToRun) == 4:\n",
    "        print(\"Plotting with 4 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        bottom4 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2])]\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        p4 = plt.bar(r, fi_list[3], bottom=bottom4, color=algColors[3], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0],p3[0],p4[0])\n",
    "        \n",
    "    elif len(algorithmsToRun) == 5:\n",
    "        print(\"Plotting with 5 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        bottom4 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2])]\n",
    "        bottom5 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3])]\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        p4 = plt.bar(r, fi_list[3], bottom=bottom4, color=algColors[3], edgecolor='white', width=barWidth)\n",
    "        p5 = plt.bar(r, fi_list[4], bottom=bottom5, color=algColors[4], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0],p3[0],p4[0],p5[0])\n",
    "        \n",
    "    elif len(algorithmsToRun) == 6:\n",
    "        print(\"Plotting with 6 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        bottom4 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2])]\n",
    "        bottom5 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3])]\n",
    "        bottom6 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4])]\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        p4 = plt.bar(r, fi_list[3], bottom=bottom4, color=algColors[3], edgecolor='white', width=barWidth)\n",
    "        p5 = plt.bar(r, fi_list[4], bottom=bottom5, color=algColors[4], edgecolor='white', width=barWidth)\n",
    "        p6 = plt.bar(r, fi_list[5], bottom=bottom6, color=algColors[5], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0],p3[0],p4[0],p5[0],p6[0])\n",
    "        \n",
    "    elif len(algorithmsToRun) == 7:\n",
    "        print(\"Plotting with 7 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        bottom4 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2])]\n",
    "        bottom5 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3])]\n",
    "        bottom6 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4])]\n",
    "        bottom7 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5])]\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        p4 = plt.bar(r, fi_list[3], bottom=bottom4, color=algColors[3], edgecolor='white', width=barWidth)\n",
    "        p5 = plt.bar(r, fi_list[4], bottom=bottom5, color=algColors[4], edgecolor='white', width=barWidth)\n",
    "        p6 = plt.bar(r, fi_list[5], bottom=bottom6, color=algColors[5], edgecolor='white', width=barWidth)\n",
    "        p7 = plt.bar(r, fi_list[6], bottom=bottom7, color=algColors[6], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0],p3[0],p4[0],p5[0],p6[0],p7[0])\n",
    "\n",
    "    elif len(algorithmsToRun) == 8:\n",
    "        print(\"Plotting with 8 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        bottom4 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2])]\n",
    "        bottom5 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3])]\n",
    "        bottom6 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4])]\n",
    "        bottom7 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5])]\n",
    "        bottom8 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5], fi_list[6])]\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        p4 = plt.bar(r, fi_list[3], bottom=bottom4, color=algColors[3], edgecolor='white', width=barWidth)\n",
    "        p5 = plt.bar(r, fi_list[4], bottom=bottom5, color=algColors[4], edgecolor='white', width=barWidth)\n",
    "        p6 = plt.bar(r, fi_list[5], bottom=bottom6, color=algColors[5], edgecolor='white', width=barWidth)\n",
    "        p7 = plt.bar(r, fi_list[6], bottom=bottom7, color=algColors[6], edgecolor='white', width=barWidth)\n",
    "        p8 = plt.bar(r, fi_list[7], bottom=bottom8, color=algColors[7], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0],p3[0],p4[0],p5[0],p6[0],p7[0],p8[0])\n",
    "        \n",
    "    elif len(algorithmsToRun) == 9:\n",
    "        print(\"Plotting with 9 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        bottom4 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2])]\n",
    "        bottom5 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3])]\n",
    "        bottom6 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4])]\n",
    "        bottom7 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5])]\n",
    "        bottom8 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5], fi_list[6])]\n",
    "        bottom9 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5], fi_list[6], fi_list[7])]\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        p4 = plt.bar(r, fi_list[3], bottom=bottom4, color=algColors[3], edgecolor='white', width=barWidth)\n",
    "        p5 = plt.bar(r, fi_list[4], bottom=bottom5, color=algColors[4], edgecolor='white', width=barWidth)\n",
    "        p6 = plt.bar(r, fi_list[5], bottom=bottom6, color=algColors[5], edgecolor='white', width=barWidth)\n",
    "        p7 = plt.bar(r, fi_list[6], bottom=bottom7, color=algColors[6], edgecolor='white', width=barWidth)\n",
    "        p8 = plt.bar(r, fi_list[7], bottom=bottom8, color=algColors[7], edgecolor='white', width=barWidth)\n",
    "        p9 = plt.bar(r, fi_list[8], bottom=bottom9, color=algColors[8], edgecolor='white', width=barWidth)\n",
    "        lines = (p1[0],p2[0],p3[0],p4[0],p5[0],p6[0],p7[0],p8[0],p9[0]) \n",
    "\n",
    "    elif len(algorithmsToRun) == 10:\n",
    "        print(\"Plotting with 10 algorithms.\")\n",
    "        bottom3 = [sum(i) for i in zip(fi_list[0], fi_list[1])]\n",
    "        bottom4 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2])]\n",
    "        bottom5 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3])]\n",
    "        bottom6 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4])]\n",
    "        bottom7 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5])]\n",
    "        bottom8 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5], fi_list[6])]\n",
    "        bottom9 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5], fi_list[6], fi_list[7])]\n",
    "        bottom10 = [sum(i) for i in zip(fi_list[0], fi_list[1], fi_list[2], fi_list[3], fi_list[4], fi_list[5], fi_list[6], fi_list[7], fi_list[8])]\n",
    "\n",
    "        p1 = plt.bar(r, fi_list[0], color=algColors[0], edgecolor='white', width=barWidth)\n",
    "        p2 = plt.bar(r, fi_list[1], bottom=fi_list[0], color=algColors[1], edgecolor='white', width=barWidth)\n",
    "        p3 = plt.bar(r, fi_list[2], bottom=bottom3, color=algColors[2], edgecolor='white', width=barWidth)\n",
    "        p4 = plt.bar(r, fi_list[3], bottom=bottom4, color=algColors[3], edgecolor='white', width=barWidth)\n",
    "        p5 = plt.bar(r, fi_list[4], bottom=bottom5, color=algColors[4], edgecolor='white', width=barWidth)\n",
    "        p6 = plt.bar(r, fi_list[5], bottom=bottom6, color=algColors[5], edgecolor='white', width=barWidth)\n",
    "        p7 = plt.bar(r, fi_list[6], bottom=bottom7, color=algColors[6], edgecolor='white', width=barWidth)\n",
    "        p8 = plt.bar(r, fi_list[7], bottom=bottom8, color=algColors[7], edgecolor='white', width=barWidth)\n",
    "        p9 = plt.bar(r, fi_list[8], bottom=bottom9, color=algColors[8], edgecolor='white', width=barWidth)\n",
    "        p10 = plt.bar(r, fi_list[9], bottom=bottom10, color=algColors[9], edgecolor='white', width=barWidth)\n",
    "\n",
    "        lines = (p1[0],p2[0],p3[0],p4[0],p5[0],p6[0],p7[0],p8[0],p9[0],p10[0]) \n",
    "        \n",
    "    # Custom X axis\n",
    "    plt.xticks(np.arange(len(all_feature_listToViz)), all_feature_listToViz,rotation='vertical')\n",
    "    plt.xlabel(\"Feature\", fontsize=20)\n",
    "    plt.ylabel(\"Normalized Feature Importance\", fontsize=20)\n",
    "    plt.legend(lines, methodsKey, loc=0,fontsize=16)\n",
    "    plt.savefig(wd_path+output_folder+'/Compare_FI_'+figName+'_'+data_name, bbox_inches = 'tight')\n",
    "    # Show graphic\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Feature Importance Plot (Normalized only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_FI_plot(top_fi_ave_norm_list,algorithmsToRun,algColors,all_feature_listToViz,'Norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fraction Adjustment of Normalized Feature importances\n",
    "- Each scores from each algorithm divided by sum of all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fracLists = []\n",
    "\n",
    "for each in top_fi_ave_norm_list:\n",
    "    fracList = []\n",
    "    for i in range(len(each)):\n",
    "        fracList.append((each[i]/(sum(each))))\n",
    "    fracLists.append(fracList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Feature Importance Plot (Normalized + Fraction Adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_FI_plot(fracLists,algorithmsToRun,algColors,all_feature_listToViz,'Norm_Frac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Performance Weighting of Normalized Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare weights\n",
    "weights = []\n",
    "\n",
    "# replace all balanced accuraces <=.5 with 0\n",
    "for i in range(len(ave_metric_list)):\n",
    "    if ave_metric_list[i] <= .5:\n",
    "        ave_metric_list[i] = 0\n",
    "\n",
    "# normalize balanced accuracies\n",
    "for i in range(len(ave_metric_list)):\n",
    "    if ave_metric_list[i] == 0:\n",
    "        weights.append(0)\n",
    "    else:\n",
    "        weights.append((ave_metric_list[i]-0.5)/0.5)\n",
    "        \n",
    "        \n",
    "# Weight normalized feature importances\n",
    "weightedLists = []\n",
    "\n",
    "for i in range(len(top_fi_ave_norm_list)):\n",
    "    weightList = np.multiply(weights[i],top_fi_ave_norm_list[i]).tolist()\n",
    "    weightedLists.append(weightList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Feature Importance Plot (Normalized + Perf. Weight Adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_FI_plot(weightedLists,algorithmsToRun,algColors,all_feature_listToViz,'Norm_Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Performance Weighting of Fract. Adj. and Normalized Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight normalized feature importances\n",
    "weightedFracLists = []\n",
    "\n",
    "for i in range(len(fracLists)):\n",
    "    weightList = np.multiply(weights[i],fracLists[i]).tolist()\n",
    "    weightedFracLists.append(weightList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Feature Importance Plot (Normalized + Frac. + Perf. Weight Adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_FI_plot(weightedFracLists,algorithmsToRun,algColors,all_feature_listToViz,'Norm_Frac_Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_elapsed_time = time.time() - notebook_start_time\n",
    "print('Pipeline Run Time: '+ str(notebook_elapsed_time))\n",
    "run_time_dict['pipeline'] = notebook_elapsed_time\n",
    "print(run_time_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
